{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xlwt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "websitesToStuff = {\n",
    "    \"https://www.auchan.ru/pokupki/eda/bakaleja.html\":\"Бакалея\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websitesToStuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current:  c3.html\n",
      "96\n",
      "current:  t6.html\n",
      "96\n",
      "current:  p6.html\n",
      "96\n",
      "current:  m4.html\n",
      "96\n",
      "current:  s1.html\n",
      "96\n",
      "current:  m3.html\n",
      "96\n",
      "current:  m2.html\n",
      "96\n",
      "current:  c6.html\n",
      "96\n",
      "current:  s4.html\n",
      "96\n",
      "current:  b1.html\n",
      "96\n",
      "current:  m5.html\n",
      "44\n",
      "current:  c5.html\n",
      "96\n",
      "current:  c7.html\n",
      "58\n",
      "current:  k2.html\n",
      "96\n",
      "current:  p9.html\n",
      "96\n",
      "current:  p10.html\n",
      "96\n",
      "current:  p16.html\n",
      "12\n",
      "current:  t3.html\n",
      "96\n",
      "current:  p1.html\n",
      "96\n",
      "current:  p5.html\n",
      "96\n",
      "current:  p7.html\n",
      "96\n",
      "current:  k5.html\n",
      "96\n",
      "current:  t8.html\n",
      "21\n",
      "current:  p3.html\n",
      "96\n",
      "current:  p8.html\n",
      "96\n",
      "current:  p12.html\n",
      "96\n",
      "current:  t1.html\n",
      "96\n",
      "current:  j1.html\n",
      "96\n",
      "current:  t4.html\n",
      "96\n",
      "current:  p2.html\n",
      "96\n",
      "current:  t5.html\n",
      "96\n",
      "current:  m1.html\n",
      "96\n",
      "current:  b2.html\n",
      "39\n",
      "current:  c1.html\n",
      "96\n",
      "current:  k1.html\n",
      "96\n",
      "current:  c4.html\n",
      "96\n",
      "current:  p13.html\n",
      "96\n",
      "current:  p14.html\n",
      "96\n",
      "current:  s5.html\n",
      "96\n",
      "current:  k4.html\n",
      "96\n",
      "current:  t7.html\n",
      "96\n",
      "current:  s2.html\n",
      "96\n",
      "current:  j2.html\n",
      "96\n",
      "current:  k3.html\n",
      "96\n",
      "current:  p11.html\n",
      "96\n",
      "current:  t2.html\n",
      "96\n",
      "current:  p4.html\n",
      "96\n",
      "current:  c2.html\n",
      "96\n",
      "current:  j3.html\n",
      "29\n",
      "current:  p15.html\n",
      "96\n",
      "current:  k6.html\n",
      "31\n",
      "current:  s7.html\n",
      "63\n",
      "current:  s6.html\n",
      "96\n",
      "current:  d1.html\n",
      "68\n",
      "current:  s3.html\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "book = xlwt.Workbook()\n",
    "sheet = book.add_sheet(\"PySheet1\")\n",
    "sheet.write(0, 0, \"Название\")\n",
    "sheet.write(0, 1, \"Цена\")\n",
    "sheet.write(0, 2, \"Описание\")\n",
    "\n",
    "\n",
    "k = 1\n",
    "#for website, stuff in websitesToStuff.items():\n",
    " #   page = requests.get(website)\n",
    "file_list = [f for f in os.listdir('.') if os.path.isfile(os.path.join('.', f)) and f.endswith('.html')]\n",
    "\n",
    "for file in file_list:\n",
    "    print(\"current: \", file)\n",
    "    with open(file) as website:\n",
    "        soup = BeautifulSoup(website, \"lxml\")\n",
    "\n",
    "    products = soup.find_all(\"div\", {\"class\",\"products-block__content\"})\n",
    "    dishes=products[0].find_all(\"a\", {\"class\", \"products__item-link\"})\n",
    "    print(len(dishes))\n",
    "\n",
    "    for dish in dishes:\n",
    "        url=dish['href']\n",
    "        page=requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        name=soup.find_all(\"div\", {\"class\",\"prcard__title\"})\n",
    "\n",
    "        fullName=soup.find_all(\"div\",{\"class\",\"breadcrumbs breadcrumbs--grey\"})\n",
    "        if fullName:\n",
    "            fullName=fullName[0].find_all(\"span\")\n",
    "            fullName=fullName[1:-1]\n",
    "            fullName.append(name[0])\n",
    "            #print(desc)\n",
    "            fullName=\"~\".join(list([x.text for x in fullName]))\n",
    "            #print(fullName)\n",
    "            sheet.write(k,0,fullName)\n",
    "            #print(description)\n",
    "        price = soup.find_all(\"div\", {\"class\",\"prcard-current-price current-price\"})\n",
    "        if price:\n",
    "            price = price[0].find_all(\"span\", {\"class\",\"price-val\"})\n",
    "            price=price[0].text\n",
    "            sheet.write(k,1,price)\n",
    "            #print(price)\n",
    "\n",
    "        desc = soup.find_all(\"div\", {\"class\",\"prcard__desc-txt\"})\n",
    "        if desc:\n",
    "            ps = desc[0].find_all(\"p\")\n",
    "            descr = \"\"\n",
    "            for p in ps:\n",
    "                descr += p.text\n",
    "            #print(descr)\n",
    "            sheet.write(k,2,descr)\n",
    "\n",
    "        k+=1\n",
    "        \n",
    "    #page = requests.get(website, verify=False)\n",
    "    #soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    #relevant = soup.find(\"div\", {\"id\": \"catalog-list\"})\n",
    "    #lines=relevant.find_all(\"div\", {\"class\", \"clearfix\"})\n",
    "\n",
    "           # print(name,price,desc)\n",
    "\n",
    "            #print(dish)\n",
    "            #break\n",
    "        #break\n",
    "    #break\n",
    "    #print(len(atr))\n",
    "\n",
    "book.save(\"ashan.xls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        print(response.body)\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuotesSpider 'quotes' at 0x7fcb05c05a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuotesSpider()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
